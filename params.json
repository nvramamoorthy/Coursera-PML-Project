{"name":"Coursera-pml-project","tagline":"This repository is for Coursera Project,  'Practical Machine Learning'   offered by Johns Hopkins Bloomberg School of of Public Health.","body":"\r\n# Coursera : Practical Machine Learning Project - Prediction Model\r\nnvramamoorthy  \r\n14 June 2015  \r\n###Synopsis\r\n\r\nThe project is about HAR - Human Activity Recognition , where 6  participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways and the relevant data were collected using the devices like  Jawbone Up, Nike FuelBand, and Fitbit which were worn by the paricipants.\r\n\r\nThe participant  normally  quantify how much of a particular activity they do, but they rarely quantify how well they do it. \r\n\r\nSix young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).\r\n\r\nThe goal is  to assess whether we could detect mistakes in weight-lifting exercises by using activity recognition techniques. we recorded users performing the same activity correctly and with a set of common mistakes with wearable sensors and used machine learning to classify each mistake. \r\n\r\nThis way, we used the training data as the activity specification and the classification algorithm as the means to compare the execution to the specification.\r\n\r\nOur aim is to build a prediction model algorithm , cross validate and find the out of sample error. \r\n\r\nWe tested the prediction model on 20 different Test cases and submitted the result as required.\r\n\r\nThe following sections deals with all data processing steps done for building the model and testing them.\r\n\r\n###Initial Setup\r\n\r\n####Environment\r\n\r\n#####Hardware : \r\n\r\n                Macbook Pro with OSX Yosemite 10.10.4\r\n\r\n#####Software : \r\n\r\n                RStudio \r\n                \r\n                GitHub DeskTop / Web Version\r\n                \r\n\r\n###Data Processing\r\n\r\n####Setting up working directory:\r\n\r\n\r\n```r\r\nsetwd(\"~/Desktop/Machine Learning/Coursera-PML-Project\")\r\n```\r\n\r\n\r\nThe required R Packeges were installed.\r\n\r\n\r\n```r\r\nlibrary(caret)\r\n```\r\n\r\n```\r\n## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n```\r\n\r\n```r\r\nlibrary(rpart)\r\nlibrary(rpart.plot)\r\nlibrary(randomForest)\r\n```\r\n\r\n```\r\n## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n```\r\n\r\n```r\r\nlibrary(corrplot)\r\n```\r\n\r\n\r\n####Download the Data    :\r\n\r\n            The training and test data were downloaded as instructed.\r\n            Trainig Data : pml-training.csv \r\n            Test data    : pml-testing.csv\r\n            \r\n\r\n\r\n```r\r\ntrainUrl <-\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\ntestUrl <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\ntrainFile <- \"./data/pml-training.csv\"\r\ntestFile  <- \"./data/pml-testing.csv\"\r\nif (!file.exists(\"./data\")) {\r\n  dir.create(\"./data\")\r\n}\r\nif (!file.exists(trainFile)) {\r\n  download.file(trainUrl, destfile=trainFile, method=\"curl\")\r\n}\r\nif (!file.exists(testFile)) {\r\n  download.file(testUrl, destfile=testFile, method=\"curl\")\r\n}\r\n```\r\n\r\n####Read the data into Dataframes and check for the content:\r\n\r\n\r\n```r\r\ntrainRaw <- read.csv(\"./data/pml-training.csv\")\r\ntestRaw <- read.csv(\"./data/pml-testing.csv\")\r\ndim(trainRaw)\r\n```\r\n\r\n```\r\n## [1] 19622   160\r\n```\r\n\r\n```r\r\ndim(testRaw)\r\n```\r\n\r\n```\r\n## [1]  20 160\r\n```\r\n\r\n####Cleaning the Data :\r\n\r\nOn inspecting both traing and test the dataset, some missing data  and unwated data were found which were to be cleaned.\r\n\r\n\r\n```r\r\nsum(complete.cases(trainRaw))\r\n```\r\n\r\n```\r\n## [1] 406\r\n```\r\n\r\nThe columns containing NA were replaced with 0:\r\n\r\n\r\n```r\r\ntrainRaw <- trainRaw[, colSums(is.na(trainRaw)) == 0] \r\ntestRaw <- testRaw[, colSums(is.na(testRaw)) == 0] \r\n```\r\nFew columns where those  data do not have any meaning with the measurements data were removed.\r\n\r\n\r\n```r\r\nclasse <- trainRaw$classe\r\ntrainRemove <- grepl(\"^X|timestamp|window\", names(trainRaw))\r\ntrainRaw <- trainRaw[, !trainRemove]\r\ntrainCleaned <- trainRaw[, sapply(trainRaw, is.numeric)]\r\ntrainCleaned$classe <- classe\r\ntestRemove <- grepl(\"^X|timestamp|window\", names(testRaw))\r\ntestRaw <- testRaw[, !testRemove]\r\ntestCleaned <- testRaw[, sapply(testRaw, is.numeric)]\r\n```\r\n\r\nRaw training data contained 19622 rows and 160 variable columns. The cleaned training data set now contains 19622 observations and 53 variables columns only, while the testing data set contains 20 observations and 53 variables. The \"classe\" variable is still in the cleaned training set.\r\n\r\n####Slicing the data:\r\n\r\nThen, we can split the cleaned training set into a pure training data set (70%) and a validation data set (30%). We will use the validation data set to conduct cross validation in future steps. \r\n\r\n\r\n```r\r\nset.seed(22519) # For Reproducible purpose\r\ninTrain <- createDataPartition(trainCleaned$classe, p=0.70, list=F)\r\ntrainData <- trainCleaned[inTrain, ]\r\ntestData <- trainCleaned[-inTrain, ]\r\n```\r\n\r\n####Data Modeling:\r\n\r\nWe fit a predictive model for activity recognition using Random Forest algorithm because it automatically selects important variables and is robust to correlated covariates & outliers in general. We will use 5-fold cross validation when applying the algorithm. \r\n\r\n\r\n```r\r\ncontrolRf <- trainControl(method=\"cv\", 5)\r\nmodelRf <- train(classe ~ ., data=trainData, method=\"rf\", trControl=controlRf, ntree=250)\r\nmodelRf\r\n```\r\n\r\n```\r\n## Random Forest \r\n## \r\n## 13737 samples\r\n##    52 predictor\r\n##     5 classes: 'A', 'B', 'C', 'D', 'E' \r\n## \r\n## No pre-processing\r\n## Resampling: Cross-Validated (5 fold) \r\n## \r\n## Summary of sample sizes: 10989, 10989, 10991, 10990, 10989 \r\n## \r\n## Resampling results across tuning parameters:\r\n## \r\n##   mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   \r\n##    2    0.9910462  0.9886729  0.001301269  0.001647776\r\n##   27    0.9914102  0.9891334  0.001717547  0.002174708\r\n##   52    0.9850037  0.9810264  0.002718384  0.003439965\r\n## \r\n## Accuracy was used to select the optimal model using  the largest value.\r\n## The final value used for the model was mtry = 27.\r\n```\r\nThe performance of the model on the validation data set is estimated.\r\n\r\n\r\n```r\r\npredictRf <- predict(modelRf, testData)\r\nconfusionMatrix(testData$classe, predictRf)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1672    1    0    0    1\r\n##          B    7 1128    4    0    0\r\n##          C    0    0 1021    5    0\r\n##          D    0    0   14  949    1\r\n##          E    0    0    1    7 1074\r\n## \r\n## Overall Statistics\r\n##                                          \r\n##                Accuracy : 0.993          \r\n##                  95% CI : (0.9906, 0.995)\r\n##     No Information Rate : 0.2853         \r\n##     P-Value [Acc > NIR] : < 2.2e-16      \r\n##                                          \r\n##                   Kappa : 0.9912         \r\n##  Mcnemar's Test P-Value : NA             \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9958   0.9991   0.9817   0.9875   0.9981\r\n## Specificity            0.9995   0.9977   0.9990   0.9970   0.9983\r\n## Pos Pred Value         0.9988   0.9903   0.9951   0.9844   0.9926\r\n## Neg Pred Value         0.9983   0.9998   0.9961   0.9976   0.9996\r\n## Prevalence             0.2853   0.1918   0.1767   0.1633   0.1828\r\n## Detection Rate         0.2841   0.1917   0.1735   0.1613   0.1825\r\n## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839\r\n## Balanced Accuracy      0.9977   0.9984   0.9903   0.9922   0.9982\r\n```\r\n\r\n\r\n```r\r\naccuracy <- postResample(predictRf, testData$classe)\r\naccuracy\r\n```\r\n\r\n```\r\n##  Accuracy     Kappa \r\n## 0.9930331 0.9911872\r\n```\r\n\r\n```r\r\noutSampErr <- 1 - as.numeric(confusionMatrix(testData$classe, predictRf)$overall[1])\r\noutSampErr\r\n```\r\n\r\n```\r\n## [1] 0.006966865\r\n```\r\n\r\nThe accuracy of the model is estimated as 99.30 %  and out of sample error of 0.69 % is estimated.\r\n\r\n####Predicting for Test Data Set:\r\n\r\nFinally  we apply the model to the original testing data set downloaded from the data source. We remove the problem_id column first. \r\n\r\n\r\n```r\r\nresult <- predict(modelRf, testCleaned[, -length(names(testCleaned))])\r\nresult\r\n```\r\n\r\n```\r\n##  [1] B A B A A E D B A A B C B A E E A B B B\r\n## Levels: A B C D E\r\n```\r\n\r\n###Conclusion\r\n\r\n#####It is found that the accuracy of the model is  99.30% and out of sample error is  0.69% .\r\n\r\nThe prediction model was tested on 20 test cases and results were submitted as required\r\n\r\nPlots are available in the folder 'writeup_files/figure-html' in th GitHub Repository and appended in Appendix.\r\n\r\nI acknowledge my thanks to the Coursera team and course participants for giving this opporunity to enrich my knowledge by doing this interesting project.\r\n\r\nI also thank HAR data provider for this excercise.\r\n\r\n###Appendix : \r\n\r\n#####Plot 1 : Decision Tree Visualization\r\n\r\n\r\n```r\r\ntreeModel <- rpart(classe ~ ., data=trainData, method=\"class\")\r\nprp(treeModel) \r\n```\r\n\r\n![](writeup_files/figure-html/unnamed-chunk-13-1.png) \r\n\r\n#####Plot 2 : Correlation Matrix Visualization\r\n\r\n\r\n```r\r\ncorrPlot <- cor(trainData[, -length(names(trainData))])\r\ncorrplot(corrPlot, method=\"color\")\r\n```\r\n\r\n![](writeup_files/figure-html/unnamed-chunk-14-1.png) \r\n\r\n\r\n\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}